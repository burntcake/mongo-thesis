\chapter{Literature Review} \label{chap:litreview}

Modern applications need to be able to grow quickly and support users across the globe. All such applications are dependent on a database backend in some shape or another. MongoDB and similar database systems promise low latency and high scalability, which are extremely attractive properties for today's application engineers. However, the distributed nature of these database systems may often result in highly unexpected behaviour - including data loss when a fraction of the participant machines experience failures. This literature review gives the reader an outline of the state-of-the-art methods for the design of distributed protocols, existing findings of database system durability failures and methods for measuring the durability of distributed database systems. In particular, we will investigate Jepsen - a tool for verifying consistency claims of distributed systems by inducing network failures. We will also inspect varying consistency models with a focus on MongoDB - an industry standard NoSQL distributed database and compare these to another well-researched system - Amazon's DynamoDB.

\section{Replication, Consensus \& Failure Models}
The fundamental property of replicated distributed systems is the ability to withstand failures of individual components while simultaneously making decisions about the global state of the system. This is achieved through the use of consensus protocols as a basis for decision making. The most well-studied approach is the \textit{Replicated State Machine} as first conceived by \citet{replicated-state-machine}. The mechanisms he described are flexible enough to be adapted to many different failure models, the most common of which are \textit{Fail-Stop} \citep{fail-stop} and \textit{Byzantine} \citep{byzantine}. 

The Fail-Stop model assumes that if a component experiences a failure, it will crash and fail to respond to any subsequent requests until it recovers. After recovery, the component is to behave normally. Handling failures under this model has been proven to be possible with $t+1$ replicas for $t$ failures, however no system has achieved this bound yet. This failure model has had much criticism levelled against it \citep{correlated-crash, gray-failure} for the oversimplified assumptions made about how a distributed system functions and the kinds of failures it experiences.

The Byzantine model, on the other hand, assumes that any component (or subset of components) can fail arbitrarily. More specifically, the model assumes that an adversary can arbitrarily partition the network for a malicious purpose and send inconsistent and conflicting messages to different components of the system. \citet{byzantine} showed that a minimum of $3t + 1$ replicas are required to handle $t$ failures under the Byzantine model. This failure model lies on the opposite side of the spectrum to Fail-Stop, getting a lot of criticism for its unrealistic and overly pessimistic assumptions about how distributed systems operate and the environments in which they operate \citep{xft, visigoth}.

These failure models provide a strong basis for a whole body of research about protocols, measurements and algorithms in distributed systems. The base observations made in these papers prove to be valuable background knowledge when analysing and understanding the protocols which build upon these models. 

\section{Distributed Protocol Design}
The basic failure models developed by \citet{fail-stop} and \citet{byzantine} have spawned much research into protocols and algorithms that allow distributed systems to function under such failures. Recently, the community has changed direction and numerous attempts have been made to refine the failure models with the aim of providing a more realistic failure model with theoretical and empirical backing.

\subsection{Paxos}

The seminal paper the field of distributed protocol design can be attributed to \textit{Paxos} \citep{paxos} - the first protocol and algorithm that allows a distributed system to withstand Byzantine failures using $2t+1$ replicas. The algorithm uses a metaphor of a part-time parliament where the legislators and the messengers could leave the parliament at any time and rejoin later arbitrarily. This metaphor maps nicely to computers, where the legislators are replicas and messages and messengers are the messages in the network. However this original algorithm is inefficient as it requires electing a new leader on every operation. Observing this, \citeauthor{paxos} also proposed \textit{multi-Paxos}, which only runs the leader election phase when the current leader fails.

Many improvements upon the classic Paxos algorithm have been made. \textit{Fast Paxos} \citep{fast-paxos} adds a new mechanism to reduce the messages needed for a decision. This comes at the cost of $3t+1$ replicas instead of $2t+1$ as with original Paxos. However Fast Paxos cannot replace the original protocol completely as there are possible scenarios where executing the "fast" variation of the protocol is impossible and a fallback to the original is required. Another improvement has been made in the form of \textit{Vertical Paxos} \citep{vertical-paxos} which allows for performing \textit{reconfiguration} - process of repairing and replacing a faulty replica with another - while simultaneously proceeding to decide on incoming commands and changes. The protocol achieves this by introducing an auxiliary master, which is in charge of deciding on reconfiguration operations and by separating the quorums for read and write operations. That is - machines in charge of deciding the value produced by a read operation will be different to machines deciding the writes. The main issues with all variations of paxos is the notorious complexity of the underlying algorithm and metaphors used to describe it. This makes the implementations of said algorithms prone to bugs and unexpected behaviours.

\subsection{Alternatives \& Improvements}
An alternative to Paxos has been proposed in the form of \textit{Raft} \citep{raft}. The authors of this protocol prove that it has the same bounds on the numbers of replicas, consistency and performance as Paxos but using techniques that are much more intuitive and have better real-world metaphors. Since MongoDB uses Raft as a basis for its consensus mechanism, it is vital to study this protocol to find how it can be leveraged to induce errors and inconsistencies in other parts of Mongo's consensus solution. Unfortunately, all follow-on work for these protocols tends to use Paxos as base, simply disregarding Raft.

For example, \textit{XFT} \citep{xft} observes that Byzantine fault-tolerance is too strong and restrictive, while Crash fault-tolerance is too weak and unrealistic for the vast majority of distributed systems. They propose to weaken the byzantine fault tolerance limitation by assuming that errors occurring are not coordinated for malicious intent, which is expected from managed systems. Using this perspective, the paper proceeds to provide a variation of Paxos called \textit{XPaxos} which can withstand $t$ byzantine failures using only $2t+1$ nodes. It proceeds with a very extensive empirical analysis of all state-of-the-art consensus algorithms, presenting data on latency and consistency for scenarios under faults and without. The authors argue that this protocol is most useful in geo-replicated systems, as getting control of effectively half the replicas across the world is infeasible. However this is a very new and novel perspective on the problem of fault tolerance, with no commercial systems having implemented this protocol and no further papers written to build on this idea.

Further work in this field also includes \textit{Visigoth} \citep{visigoth} which is a fault-tolerance protocol designed for data centers. The authors begin with the assertion that BFT models are unnecessarily conservative for data center environments, backing up the claim by observing that BFT is designed to cope with coordinated malice, which is unlikely to happen within the security perimeter of the data centre. Further, it observes that data centers have reliable and predictable network connectivity, meaning that the assumption of full asynchronicity is also too restrictive. Visigoth provides a framework to adjust the level of asynchrony and byzantine-ness of a replica set as to improve performance, reduce network overhead and reduce the number of replicas required to $2t+1$ for arbitrary errors. Since we know that MongoDB can be deployed on premises and in a data-center, so seeing its behaviour under both conditions and comparing it to VFT's guarantees and performance can offer insight into whether adopting this failure model or providing such an option would improve its performance and durability guarantees. Similarly to \textit{XFT}, there are no papers that build on this framework, making it difficult to evaulate the impact of this framework.

Another paper that tries to tackle this issue is \textit{ThriftyPaxos} \citep{thriftypaxos} which observes that using \textit{on-demand instantiation} \citep{cheappaxos, zz} and \textit{lazy recovery} \citep{lazyrep, lazyrep-availability} can allow distributed systems to handle byzantine failures more efficiently, using only $2t+1$ replicas. The key observation driving this approach is that logical separation of \textit{agreement nodes} and \textit{execution nodes} allows for lazy recovery. The authors propose a variation of Paxos - ThriftyPaxos which incorporates the logical separation of nodes into its execution. The paper evaluates its performance in comparison to Paxos and some CFT algorithms for performance, availability and adaptive recovery, however provides no empirical data for live distributed systems.

Attempting to solve this problem in a completely different approach comes from \textit{NOPaxos} \citep{consensus-network-ordering} which proposes an overhaul of how this problem should be tackled. Instead of trying to deal with unordered messages in an asynchronous environment at the application layer, like Paxos and Raft, the authors deal with these issues on different layers. Firstly, they design a new network primitive - \text{Ordered Unreliable Multicast}  where all receivers are guaranteed to process multicast messages in the same order, but messages may be lost. The paper proceeds to show multiple ways of implementing this primitive, showing that it has no impact on network performance when implemented using datacenter hardware. Secondly, they introduce NOPaxos - a consensus algorithm utilising the OUM. It requires application-level coordination only to handle dropped packets, a fundamentally simpler problem than ordering requests. The resulting protocol is compared to other state-of-the-art consensus methods and is found to be simpler, achieving throughput $\sim$5x that of other protocols - comparable to an equivalent unreplicated system. 

\subsection{Summary}
Observations made in Paxos and its alternatives make for a very extensive body of work on which to base how MongoDB will respond to failures. This knowledge will be vital in designing a standardised durability testing framework for measuring the guarantees MongoDB claims. This is especially important as many of the protocols proposed above do not provide empirical evidence in a comparable way, hence making it very difficult to evaluate their performance, benefits and drawbacks against one another.

\section{Failure Analysis}
Due to the very dynamic and fast-paced nature of this field, numerous papers have been published finding bugs and inconsistencies in the general operation of replicated database systems such as MongoDB or proposing new ways to observe, analyse and mitigate errors and failures in distributed systems.

One of the key papers in this field is \textit{Correlated Crash Vulnerabilities} \citep{correlated-crash} which observes that, generally, replicas in a set will not all crash at once, and at least some of the replicas are correct at any point in time. It proceeds to consider a scenario in which all replicas of a particular data shard crash at the same time and recover at a later point and calls this a \textit{correlated failure}.  It proceeds by introducing PACE - a novel framework for exploring correlated crash vulnerabilities. It also shows that different filesystems will affect the susceptibility of a system to correlated crash vulnerabilities. Using these findings, the paper analyses many popular distributed databases for correlated crash vulnerabilities, finding 1 on MongoDB using WiredTiger engine and 5 on MongoDB using Rocks engine. This paper provides a great foundation upon which to build further research in this area, specifically investigating other forms of correlated crashes. However, this paper only assumes a \textit{Crash Failure Model} which is not fully representative of many production systems.

The \textit{Gray Failure} \citep{gray-failure} paper introduces a new perspective on how distributed systems misbehave. The authors observe that most severe system outages are caused when a component of a system does not notice a fault but still gets affected by it. It derives a definition for such failures as Gray Failures and observes that their key property is \textit{differential observability}, where one entity is negatively affected by the failure and another entity does not perceive the failure. Arguing that Byzantine Fault-Tolerant solutions are overkill, the authors use the differential observability definition to define practices on how to handle gray failures. Specifically, it notes that gray failures can only be detected in a distributed fashion - as different components have different views of the system and aggregating those results would provide a more holistic view of the system health. Unfortunately, the paper lacks empirical evidence to back up its claims. Its observations of gray failures could become a very valuable tool to test how MongoDB would handle such failures.

\subsection{Summary}
While this field of research tends to lack in depth and breadth, the observations made within these papers provide a vital perspective on further analysing and discovering theoretical and practical failures in industry-standard replicated database systems. Specifically, the lack of a standardised measure or framework makes analysing the severity of these failures difficult to analyse and compare.

\section{Durability Metrics}
There are few metrics designed specifically to measure the durability of a distributed system. Most similar metrics revolve around measuring the probability of data loss of RAID arrays. This only further signifies the need for a standard benchmark of reliability.

The standard way of measuring and reporting the reliability \textit{MTTDL} \citep{mttdl}. This measurement uses the scale of the system, the reliability of its components and the rate of recovery to determine how long the system may last without data loss. Unfortunately, this measurement has come under a lot of criticism for its overly optimistic estimates and incomparable results. For example, \citep{mttdl-meaningless} show that modern archive storage systems can achieve MTTDL of 1400 years, which is obviously well beyond the lifetime of any system. They further propose NOMDL - Normalised Magnitude of Data Loss. This measurement attempts to compensate for the shortcomings of MTTDL by measuring the amount of data loss in bytes over a fixed period of time $t$ per usable terabyte of storage. The authors argue that NOMDL is more meaningful, understandable and comparable than MTTDL, BHL \citep{bit-preservation} and DDF pKRG \citep{enhanced-reliability-raid} while being insensitive to technological change.

Further work in this field includes \textit{Network Aware Reliability Analysis} \citep{network-aware-reliability} which notes that the task of accurately measuring reliability of a distributed system is intractable using current methods. The main reason behind this is because very low-probability failure events create a large impact on the reliability measure. The authors further claim that existing approaches suffer from unrealistic expectations regarding network bandwidth. The paper proceeds by proposing a new measurement framework with the focus on the cumulative effect of simultaneous failures on repair time, taking the network bandwidth into account. They also note that a node's repair time is a function of disk bandwidth \textit{and} network bandwidth, which - when taken into account - increases the variance in the probability of data loss estimates by two to four orders of magnitude, hence providing a more accurate perspective on the problem of data loss in a distributed replicated system. The observations in this paper present a vital perspective which can be used to tackle similar questions and perform relevant measurements when attempting to induce errors in MongoDB.

\subsection{Summary}
The metrics found in the literature tend to be centered around single machines with multiple disks configured in a RAID array. There is a gap in these metrics when distributed systems are introduced, making the development of a consistent, comparable and meaningful durability metric for a replicated storage system an important task which would open many pathways for further research.

\section{Consistency Models}
Consistency models provide ways to verify whether a storage system is behaving correctly under the limitations and guarantees imposed by the model.

The most successful model for consistency originated as a result of work in relational databases. \citet{acid} define the ACID consistency model, which states that every database transaction must be \textit{Atomic, Consistent, Isolated \& Durable}. This means that each transaction must complete in full or not at all; updates to data must keep the database in a valid state; the effects of concurrent transactions will be the same if those transactions were serialized; and once a transaction is committed, the change will not be lost. 

\citet{cap} conjectured that it is impossible for a distributed data storage system to satisfy \textit{Consistency, Availability \& Partition Tolerance} simultaneously. This was later proved by \citet{cap-proof}. The resultant CAP theorem therefore limits the capabilities of a theoretically optimal distributed system to either guaranteeing consistency or availability since we cannot sacrifice partition tolerance in a distributed system. \citet{cap-years-after} revisits the theorem and admits that CAP is misleading as it oversimplifies the relation between the three properties as binary - either the property is satisfied completely or not at all. 

As a result, \citet{pacelc} builds on the CAP theorem, creating PACELC. He adds that further tradeoffs need to be made between latency and consistency even when no network partitions exist. The main advantage of this model is its more complete portrayal of how distributed systems operate in the presence of network partitions and without.

\textit{BASE} \citep{base} is the distributed systems' alternative to ACID. It is the conceptual equivalent to the CAP tradeoff of Consistency for Availability. The model states that the system should be \textit{Basically Available, Soft state \& Eventually Consistent}. \citet{eventual-consistency} defines eventual consistency as servers \textit{converging} to the same state in the absence of updates. As such, BASE provides us with a perspective on the favourable properties and behaviors of a distributed system.

\subsection{Summary}
The consistency models of data storage systems have seen many iterations through the lifetime of the field. The shift in perspective from strongly consistent systems to eventually consistency has given rise to highly available, scalable and performant distributed systems. While much work has been done on defining the models and studying their theoretical limits, little progress has been made in the empirical evaluation of the models, especially how the durability of a system affects its consistency and latency guarantees.

\section{MongoDB}
\label{sec:lit-mongo}
MongoDB is an industry-standard NoSQL document store with capabilities for distributed sharding and replication.  It uses a custom method for consensus on replica sets using a modification of the Primary-Backup replication model via a combination of Raft for leader election and an operations log (Oplog). 

The Oplog keeps a record of all operations that modify the data stored in the database. The elected leader will push its oplog changes to the secondary MongoDB nodes in order to make the data consistent across all replicas.

It is commonly found in literature in comparisons with other NoSQL databases, mostly for performance. The work regarding durability guarantees of MongoDB or other NoSQL document stores is severely lacking, highlighting the need for a standard benchmark to test the claims these systems make about durability.

MongoDB has long been a source of critique and research of database properties in distributed systems. \citet{jepsen-2013} was the first to perform an evaluation of MongoDB's consistency properties, finding that it lost all acknowledged writes at all consistency levels when a network failure occurs. MongoDB has since been fixing these errors and improving their guarantees, however some violations still persist \citep{jepsen-2018}. Considering the seeming complexity of ensuring consistency properties of MongoDB, we should also address any possible failures present in its durability properties.

\section{Dynamo}
Amazon's Dynamo \citep{dynamo} is a classic distributed key-value storage system that has fundamentally changed the way we reason about data storage at large scale. Its influential ideas and principles paved the way for the entire field of NoSQL database research and development and resulted in the development of more complex AWS services such as DynamoDB \citep{dynamodb} and Aurora \citep{aurora}.

The authors of this paper introduced a revolutionary multi-master replication scheme which traded strong consistency for availability and performance. The basic principle of the scheme involves making each node responsible for a chunk of the overall data and being able to reroute a request to the correct replica in a single step. To satisfy availability guarantees, data is replicated across multiple nodes across the entire system, with synchronisation of state between replicas being handled by a quorum-based protocol.

These design tradeoffs allowed Amazon to achieve performance levels that would have been impossible with traditional relational databases, providing for a compelling case-study of the limitations and tradeoffs between relational and NoSQL storage solutions.

\section{Jepsen}
Jepsen is an open-source software library designed to test consistency claims of distributed systems. Jepsen is also used to simulate certain extreme failure scenarios and observe how the system tolerates and recovers from these. Jepsen achieves this by performing a series of operations on the distributed system and analysis whether the execution of this operation satisfies certain consistency properties.

The most notable work with Jepsen on MongoDB was done by \citeauthor{jepsen-2017} and \citeauthor{jepsen-2018} \citep{jepsen-2017, jepsen-2018}, who performed several analyses of MongoDB's consistency claims and found a variety of violations caused by inducing network errors. Unfortunately, Jepsen does not have the capability to induce local errors on a single replica, which prevents it from being used to test and measure the durability of these systems.

\section{Conclusion}
We have studied the literature regarding distributed systems with a focus on replication \& consensus protocols, failures, durability metrics and consistency models. We have investigated MongoDB and how it achieves replication. We have also observed that the literature in these fields is very extensive on theory but empirical analysis is lacking. There is a clear absence of a standardised metric or framework for durability of distributed data storage systems which makes it impossible to fully evaluate the tradeoffs between different systems.




